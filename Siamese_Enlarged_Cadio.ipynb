{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"siamese","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPxqPs2wjNSzo+qG7TCogVy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1FNN1D2aKPSX"},"source":["import os\n","import numpy as np\n","import time\n","import sys\n","import csv\n","import cv2\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn.functional as tfunc\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataset import random_split\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from PIL import Image\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics.ranking import roc_auc_score\n","import sklearn.metrics as metrics\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wmCZuhKKVO7"},"source":["image_set_csv = 'CheXpert-v1.0-small/train.csv'\n","image_set = 'CheXpert-v1.0-small/train'\n","\n","test_set_csv = 'CheXpert-v1.0-small/valid.csv'\n","test_set = 'CheXpert-v1.0-small/valid'\n","\n","use_gpu = torch.cuda.is_available()\n","\n","pretrained = True                \n","classes = 14     \n","batch_size = 16\n","epochs = 20\n","resize_im = 224\n","#'Enlarged Cardiomediastinum', 'Pneumonia', 'Atelectasis', \n","\n","# class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n","#                'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n","#                'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n","\n","class_names = ['Enlarged Cardiomediastinum']\n","# pathology = 'Atelectasis'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmSiOtsXdZmb"},"source":["df = pd.read_csv(image_set_csv)\n","# df=df.fillna(0)\n","# df_ran = df[['Path','Enlarged Cardiomediastinum']]\n","# print(df[['Path','Enlarged Cardiomediastinum']].head())\n","# print(len(df[df['Enlarged Cardiomediastinum']==-1]))\n","# print(df_ran.iloc[1,0])\n","# # df=df.fillna(0)\n","# # label = int(df.loc[df['Path']=='CheXpert-v1.0-small/train/patient29525/study7/view1_frontal.jpg','Enlarged Cardiomediastinum'])\n","# # print(label)\n","# # print(type(label))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBfkZaVmKYzf"},"source":["X = pd.read_csv(image_set_csv)\n","X = X.fillna(0)\n","# print(X.shape)\n","# print(X.loc[:,X.columns!=pathology].head())\n","X_train, X_test, y_train, y_test = train_test_split(X.loc[:,X.columns!=pathology], X[pathology], test_size=0.20, random_state=42,stratify = X[pathology])\n","print(len(X_train))\n","print(len(X_test))\n","X = X_train.join(y_train)\n","print(X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DFBjisaKatI"},"source":["class Siamese_CheXpertDataSet(Dataset):\n","    def __init__(self, image_set_csv, image_set,set_type, pathology, transform=None):\n","        \"\"\"\n","        image_list_file: path to the file containing images with corresponding labels.\n","        transform: optional transform to be applied on a sample.\n","        \"\"\"\n","        df_train = pd.read_csv(image_set_csv)\n","        df_train = df_train.fillna(0)\n","        if set_type == 'train' or set_type == 'val':\n","            X_train, X_val, y_train, y_val = train_test_split(df_train.loc[:,df_train.columns!=pathology], df_train[pathology], test_size=0.20, random_state=42, stratify = df_train[pathology])\n","            X_train = X_train.join(y_train)\n","            X_val = X_val.join(y_val)\n","\n","        if set_type == 'train':\n","            self.df = X_train[['Path',pathology]]\n","            self.images_names = list(X_train[\"Path\"])\n","            \n","        elif set_type == 'val':\n","            self.df = X_val[['Path',pathology]]\n","            self.images_names = list(X_val[\"Path\"])\n","        else: \n","            self.df = df_train[['Path',pathology]]\n","            self.labels = list(df_train[pathology])\n","            self.images_names = list(df_train[\"Path\"])\n","\n","        self.transform = transform\n","        self.set_type = set_type\n","\n","    def __getitem__(self, index):\n","        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n","        #Assuming -1 are equivalent to 0\n","        df_ones = self.df[self.df[pathology]==1]\n","        df_ones = pd.concat([df_ones, self.df[self.df[pathology]==-1]])\n","        df_others = self.df[self.df[pathology]==0]\n","        index = np.random.randint(0,len(self.images_names))\n","        same_class = np.random.randint(0,2) \n","        anchor_class = np.random.randint(0,2) \n","        #image_names = df_train[:,[pathology]] == 1\n","        # .5 for uncertainty?\n","        if self.set_type == 'val' or self.set_type == 'train':\n","            if same_class:\n","                if anchor_class:\n","                    image = np.random.choice(df_ones['Path'],2)\n","                    image_1 = image[0]\n","                    image_2 = image[1]\n","                else:\n","                    image = np.random.choice(df_others['Path'],2)\n","                    image_1 = image[0]\n","                    image_2 = image[1]\n","                label = 1\n","            else:\n","                image_1 = np.random.choice(df_ones['Path'])\n","                image_2 = np.random.choice(df_others['Path'])\n","                label = 0\n","\n","            image_1 = Image.open(image_1).convert('RGB')\n","            image_2 = Image.open(image_2).convert('RGB')\n","            if self.transform is not None:\n","                image_1 = self.transform(image_1)\n","                image_2 = self.transform(image_2)\n","            return image_1, image_2, label\n","    \n","        else:\n","            image_name = self.images_names[index]\n","            image = Image.open(image_name).convert('RGB')\n","            label = self.labels[index]\n","            if self.transform is not None:\n","                image = self.transform(image)\n","            return image, label\n","\n","\n","    def __len__(self):\n","        return len(self.images_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUO2W6EyKcy-"},"source":["normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","transformList= []\n","transformList.append(transforms.RandomVerticalFlip())\n","transformList.append(transforms.Resize((resize_im, resize_im)))\n","transformList.append(transforms.ToTensor())\n","transformList.append(normalize)      \n","transformSequence_train=transforms.Compose(transformList)\n","\n","transformList= []\n","transformList.append(transforms.Resize((resize_im, resize_im)))\n","transformList.append(transforms.ToTensor())\n","transformList.append(normalize)      \n","transformSequence_val=transforms.Compose(transformList)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcqN-VSBKeOB"},"source":["pathology = 'Enlarged Cardiomediastinum'\n","datasetTrain = Siamese_CheXpertDataSet(image_set_csv,image_set,'train',pathology,transformSequence_train)\n","datasetVal = Siamese_CheXpertDataSet(image_set_csv,image_set,'val',pathology,transformSequence_val)\n","datasetTest = Siamese_CheXpertDataSet(test_set_csv,test_set,'test',pathology,transformSequence_val)\n","\n","dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=batch_size, shuffle=True,  num_workers=24, pin_memory=True)\n","dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n","dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=1, shuffle=False, num_workers=24, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1fv6mYSKfld"},"source":["print(len(dataLoaderTrain))\n","print(len(dataLoaderVal))\n","print(len(dataLoaderTest))\n","print(len(datasetTrain))\n","print(len(datasetVal))\n","print(len(datasetTest))\n","# print(np.random.randint(0,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"efSfpo8SKhkv"},"source":["def imshow(img,text=None,should_save=False):\n","    npimg = img.numpy()\n","    plt.axis(\"off\")\n","    if text:\n","        plt.text(75, 8, text, style='italic',fontweight='bold',\n","            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()  \n","dataiter = iter(dataLoaderTrain)\n","\n","example_batch = next(dataiter)\n","concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n","imshow(torchvision.utils.make_grid(concatenated))\n","print(example_batch[2].numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pllKpji8KrA_"},"source":["class ContrastiveLoss(torch.nn.Module):\n","\n","      def __init__(self, margin):\n","            super(ContrastiveLoss, self).__init__()\n","            self.margin = margin\n","            self.eps = 1e-9\n","\n","      def forward(self, output1, output2, label):\n","#             print(output1.shape)\n","#             print(output2.shape)\n","#             print(label.shape)\n","#             print(label)\n","            # Find the pairwise distance or eucledian distance of two output feature vectors\n","            euclidean_distance = F.pairwise_distance(output1, output2)\n","            # perform contrastive loss calculation with the distance\n","            loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n","            (label) * torch.pow(torch.clamp(self.margin - (euclidean_distance+self.eps), min=0.0), 2))\n","\n","            return loss_contrastive\n","        \n","loss = ContrastiveLoss(margin=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mbcTxh_Ks30"},"source":["class Siamese_DenseNet121(nn.Module):\n","    def __init__(self, classNum, pretrained):\n","\n","        super(Siamese_DenseNet121, self).__init__()\n","\n","        self.densenet121 = torchvision.models.densenet121(pretrained=pretrained, memory_efficient=True)\n","        prevNum = self.densenet121.classifier.in_features\n","        #replacing classifier FC for 1 class\n","        self.densenet121.classifier = nn.Sequential(nn.Linear(prevNum, classNum), nn.ReLU())\n","\n","    def forward_once(self, x):\n","        x = self.densenet121(x)\n","        return x\n","\n","    def forward(self, input1, input2):\n","        output1 = self.densenet121(input1)\n","        output2 = self.densenet121(input2)\n","        return output1, output2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kkz7zLatKzHb"},"source":["class Siamese_ChexpertTrainer():\n"," \n","    def train (dataloaderTrain, dataloaderVal, pretrained, classes, batch_size, epoch, resize_im, launchTimestamp, checkpoint):\n","\n","        \n","        #-------------------- SETTINGS: NETWORK ARCHITECTURE\n","        #model = DenseNet121(classes, pretrained).cuda()\n","        model = Siamese_DenseNet121(classes, pretrained).cuda()\n","        # model = torch.nn.DataParallel(model).cuda()\n","\n","        #-------------------- SETTINGS: OPTIMIZER & SCHEDULER\n","        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n","        scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n","        \n","        #---- Load checkpoint \n","        if checkpoint != None and use_gpu:\n","            modelCheckpoint = torch.load(checkpoint)\n","            model.load_state_dict(modelCheckpoint['state_dict'])\n","            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n","\n","        \n","        #---- TRAIN THE NETWORK\n","        \n","        lossMIN = 100000\n","        \n","        for epochID in range (0, epoch):\n","            \n","            timestampTime = time.strftime(\"%H%M%S\")\n","            timestampDate = time.strftime(\"%d%m%Y\")\n","            timestampSTART = timestampDate + '-' + timestampTime\n","                         \n","            Siamese_ChexpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, scheduler, epoch, classes, loss)\n","            lossVal, losstensor = Siamese_ChexpertTrainer.epochVal (model, dataLoaderVal, optimizer, scheduler, epoch, classes, loss)\n","            \n","            timestampTime = time.strftime(\"%H%M%S\")\n","            timestampDate = time.strftime(\"%d%m%Y\")\n","            timestampEND = timestampDate + '-' + timestampTime\n","            \n","            scheduler.step(losstensor)\n","            \n","            if lossVal < lossMIN:\n","                lossMIN = lossVal    \n","                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'siamese_models/'+'m-' + launchTimestamp + '.pth.tar')\n","                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n","            else:\n","                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n","                     \n","    #-------------------------------------------------------------------------------- \n","       \n","    def epochTrain (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n","        \n","        model.train()\n","        \n","        for batchID, (input_1, input_2, target) in enumerate (dataLoader):\n","                        \n","            input_1 = Variable(input_1).cuda()\n","            input_2 = Variable(input_2).cuda()\n","            target = Variable(target).cuda()\n","            \n","            optimizer.zero_grad()\n","                         \n","            varOutput_1,varOutput_2 = model.forward(input_1,input_1)\n","            lossvalue = loss(varOutput_1,varOutput_2,target)       \n","            lossvalue.backward()\n","            optimizer.step()\n","            \n","    #-------------------------------------------------------------------------------- \n","        \n","    def epochVal (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n","        \n","        model.eval()\n","        \n","        lossVal = 0\n","        lossValNorm = 0\n","        \n","        losstensorMean = 0\n","        \n","        with torch.no_grad(): \n","            \n","            for i, (input_1,input_2, target) in enumerate (dataLoader):\n","\n","                input_1 = Variable(input_1).cuda()\n","                input_2 = Variable(input_2).cuda()\n","                target = Variable(target).cuda()\n","\n","                varOutput_1,varOutput_2 = model.forward(input_1,input_1)\n","\n","                losstensor = loss(varOutput_1,varOutput_2,target)\n","                losstensorMean += float(losstensor)\n","\n","                lossVal += float(losstensor) #or losstensor.item()\n","                lossValNorm += 1\n","\n","            outLoss = lossVal / lossValNorm\n","            losstensorMean = losstensorMean / lossValNorm\n","\n","            return outLoss, losstensorMean\n","    \n","    def computeAUROC (dataGT, dataPRED):\n","        \n","        return roc_auc_score(dataGT, dataPRED)\n","\n","            \n","    def test (dataloaderTest, pathModel, classes, pretrained, batch_size, resize_im, launchTimeStamp, class_names, train_set, transform):   \n","        \n","        cudnn.benchmark = True\n","\n","        df_ones = train_set[train_set[pathology]==1].head(3)\n","        df_minus_ones = train_set[train_set[pathology]==-1].head(3) \n","        df_others = train_set[train_set[pathology]==0].head(3)\n","        \n","        #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n","        model = Siamese_DenseNet121(classes, pretrained).cuda()\n","        \n","        modelCheckpoint = torch.load(pathModel)\n","        model.load_state_dict(modelCheckpoint['state_dict'])\n","        \n","        outGT = []\n","        outPRED = []\n","        outPRED_ones = 0\n","        outPRED_zeros = 0\n","       \n","        model.eval()        \n","\n","        for i, (input_2, target) in enumerate(dataLoaderTest):\n","            \n","            outGT.extend(target.numpy())\n","            \n","            if i%50 == 0:\n","                print(\"i\")\n","                print(i)\n","            \n","            \n","            for j in range(3):\n","\n","                input_1 = transform(Image.open(df_ones.iloc[j,0]).convert('RGB'))\n","                input_3 = transform(Image.open(df_minus_ones.iloc[j,0]).convert('RGB'))\n","                input_4 = transform(Image.open(df_others.iloc[j,0]).convert('RGB'))\n","\n","                input_1 = Variable(input_1).cuda()\n","                input_2 = Variable(input_2).cuda()\n","                input_3 = Variable(input_3).cuda()\n","                input_4 = Variable(input_4).cuda()\n","                bs, c, h, w = input_2.size()\n","                input_1 = input_1.view(-1, c, h, w)\n","                input_2 = input_2.view(-1, c, h, w)\n","                input_3 = input_3.view(-1, c, h, w)\n","                input_4 = input_3.view(-1, c, h, w)\n","                \n","#                 print(type(target.numpy()))\n","#                 print(type(Variable(target).float()))\n","#                 print(type(outGT))\n","\n","                out_1,out_2 = model(input_1,input_2)\n","                out = F.pairwise_distance(out_1, out_2)\n","#                 print(type(out))\n","#                 print(type(out.item()))\n","#                 print(out.shape)\n","                outPRED_ones += out.item()\n","\n","                out_1,out_2 = model(input_3,input_2)\n","                out = F.pairwise_distance(out_1, out_2)\n","                outPRED_ones += out.item()\n","\n","                out_1,out_2 = model(input_4,input_2)\n","                out = F.pairwise_distance(out_1, out_2)\n","#                 print(out.shape)\n","                outPRED_zeros += out.item()\n","            \n","            outPRED_ones = np.exp(outPRED_ones) / (np.exp(outPRED_ones)+np.exp(outPRED_zeros))\n","            outPRED_zeros = np.exp(outPRED_zeros) / (np.exp(outPRED_ones)+np.exp(outPRED_zeros))\n","            \n","#             print(\"outpred_ones\")\n","#             print(outPRED_ones)\n","            \n","            if outPRED_ones > outPRED_zeros:\n","                outPRED.append(1)\n","                #outPRED = torch.cat((outPRED, 1), 0)\n","            else:\n","                outPRED.append(0)\n","                #outPRED = torch.cat((outPRED, 0), 0)\n","        \n","        print(len(outPRED))\n","        print(len(outGT))\n","        print(outPRED[0])\n","        print(outGT[0])\n","\n","        aurocIndividual = Siamese_ChexpertTrainer.computeAUROC(outGT, outPRED)\n","        aurocMean = np.array(aurocIndividual).mean()\n","        \n","        print ('AUROC mean ', aurocMean)\n","        \n","#         for i in range (0, len(aurocIndividual)):\n","#             print (class_names[i], ' ', aurocIndividual[i])\n","        \n","        return outGT, outPRED, aurocMean\n","#-------------------------------------------------------------------------------- "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLitYP9jLPGV"},"source":["timestampTime = time.strftime(\"%H%M%S\")\n","timestampDate = time.strftime(\"%d%m%Y\")\n","timestampLaunch = timestampDate + '-' + timestampTime\n","       \n","pathModel = 'm-' + timestampLaunch + '.pth.tar'\n","    \n","print ('Training NN architecture')\n","Siamese_ChexpertTrainer.train(dataLoaderTrain, dataLoaderVal, pretrained, classes, batch_size, epochs, resize_im, timestampLaunch, None)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjcP0f-dLRHZ"},"source":["print(os.listdir('siamese_models/'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VYe7UlVLS-c"},"source":["pathModel = 'siamese_models/m-30112020-044136.pth.tar'\n","print ('Testing the trained model')\n","outGT1, outPRED1, auc = Siamese_ChexpertTrainer.test(dataLoaderTest, pathModel, classes, pretrained, batch_size, resize_im, timestampLaunch, class_names,df, transformSequence_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YastuXYidvs6"},"source":["false_rate, true_rate, threshold = metrics.roc_curve(outGT1, outPRED1)\n","roc_auc = metrics.auc(false_rate, true_rate)\n","i=0    \n","plt.title('ROC for: ' + class_names[i])\n","plt.plot(false_rate, true_rate, label = 'U-ones: AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","\n","plt.savefig('Enlarged.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jz-OpDXHdxNt"},"source":["plt.hist(outGT1)\n","plt.show()\n","plt.hist(outPRED1)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFteGbzfdyhQ"},"source":["from sklearn.metrics import confusion_matrix\n","\n","labels = ['negative','positive']\n","cm = confusion_matrix(outGT1, outPRED1)\n","print(cm)\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(cm,cmap=plt.cm.Blues)\n","plt.title('Confusion matrix for Enlarged Cardiomediastinum')\n","fig.colorbar(cax)\n","ax.set_xticklabels([''] + labels)\n","ax.set_yticklabels([''] + labels)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.savefig('confusion_Enlarged Cardiomediastinum.png')\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}