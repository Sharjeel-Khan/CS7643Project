{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network - Multiclass Label Integration for 'Pneumonia' and 'Atelectasis'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import torchvision\n",
    "\n",
    "\n",
    "# import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_csv = 'CheXpert-v1.0-small/train.csv'\n",
    "image_set = 'CheXpert-v1.0-small/train'\n",
    "\n",
    "test_set_csv = 'CheXpert-v1.0-small/valid.csv'\n",
    "test_set = 'CheXpert-v1.0-small/valid'\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "pretrained = True                \n",
    "classes = 14     \n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "resize_im = 224\n",
    "#'Enlarged Cardiomediastinum', 'Pneumonia', 'Atelectasis', \n",
    "\n",
    "# class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "#                'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "#                'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "\n",
    "# class_names = ['Pneumonia']\n",
    "# pathology = 'Pneumonia'\n",
    "\n",
    "class_names = ['Atelectasis']\n",
    "pathology = 'Atelectasis'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(image_set_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(image_set_csv)\n",
    "X = X.fillna(0)\n",
    "# print(X.shape)\n",
    "# print(X.loc[:,X.columns!=pathology].head())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.loc[:,X.columns!=pathology], X[pathology], test_size=0.20, random_state=42,stratify = X[pathology])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "X = X_train.join(y_train)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_CheXpertDataSet(Dataset):\n",
    "    def __init__(self, image_set_csv, image_set,set_type, pathology, transform=None):\n",
    "        \"\"\"\n",
    "        image_list_file: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        df_train = pd.read_csv(image_set_csv)\n",
    "        df_train = df_train.fillna(0)\n",
    "        if set_type == 'train' or set_type == 'val':\n",
    "            X_train, X_val, y_train, y_val = train_test_split(df_train.loc[:,df_train.columns!=pathology], df_train[pathology], test_size=0.20, random_state=42, stratify = df_train[pathology])\n",
    "            X_train = X_train.join(y_train)\n",
    "            X_val = X_val.join(y_val)\n",
    "\n",
    "        if set_type == 'train':\n",
    "            self.df = X_train[['Path',pathology]]\n",
    "            self.images_names = list(X_train[\"Path\"])\n",
    "            \n",
    "        elif set_type == 'val':\n",
    "            self.df = X_val[['Path',pathology]]\n",
    "            self.images_names = list(X_val[\"Path\"])\n",
    "        else: \n",
    "            self.df = df_train[['Path',pathology]]\n",
    "            self.labels = list(df_train[pathology])\n",
    "            self.images_names = list(df_train[\"Path\"])\n",
    "\n",
    "        self.transform = transform\n",
    "        self.set_type = set_type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        df_ones = self.df[self.df[pathology]==1]\n",
    "        self.df[self.df[pathology] == -1] = 2\n",
    "        df_twos = self.df[self.df[pathology] == 2]\n",
    "        df_others = self.df[self.df[pathology]==0]\n",
    "        \n",
    "        \n",
    "        index = np.random.randint(0,len(self.images_names))\n",
    "        same_class = np.random.randint(0,2) \n",
    "        diff_class = np.random.randint(0,3)\n",
    "        anchor_class = np.random.randint(0,3) \n",
    "        \n",
    "        # multi-class item distribution to provide to Siamese Network\n",
    "        if self.set_type == 'val' or self.set_type == 'train':\n",
    "            if same_class:\n",
    "                if anchor_class == 0:\n",
    "                    image = np.random.choice(df_ones['Path'],2)\n",
    "                    image_1 = image[0]\n",
    "                    image_2 = image[1]\n",
    "                elif anchor_class == 1:\n",
    "                    image = np.random.choice(df_others['Path'],2)\n",
    "                    image_1 = image[0]\n",
    "                    image_2 = image[1]\n",
    "                elif anchor_class == 2:\n",
    "                    image = np.random.choice(df_twps['Path'], 2)\n",
    "                    image_1 = image[0]\n",
    "                    image_2 = image[1]\n",
    "                label = 1\n",
    "            else:\n",
    "                if anchor_class == 0:\n",
    "                    image_1 = np.random.choice(df_ones['Path'])\n",
    "                    image_2 = np.random.choice(df_twos['Path'])\n",
    "                    label = 0\n",
    "                elif anchor_class == 1:\n",
    "                    image_1 = np.random.choice(df_others['Path'])\n",
    "                    image_2 = np.random.choice(df_twos['Path'])\n",
    "                    label = 0\n",
    "                elif anchor_class == 2:\n",
    "                    image_1 = np.random.choice(df_ones['Path'])\n",
    "                    image_2 = np.random.choice(df_others['Path'])\n",
    "                    label = 0\n",
    "\n",
    "            image_1 = Image.open(image_1).convert('RGB')\n",
    "            image_2 = Image.open(image_2).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image_1 = self.transform(image_1)\n",
    "                image_2 = self.transform(image_2)\n",
    "            return image_1, image_2, label\n",
    "    \n",
    "        else:\n",
    "            image_name = self.images_names[index]\n",
    "            image = Image.open(image_name).convert('RGB')\n",
    "            label = self.labels[index]\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList= []\n",
    "transformList.append(transforms.RandomVerticalFlip())\n",
    "transformList.append(transforms.Resize((resize_im, resize_im)))\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence_train=transforms.Compose(transformList)\n",
    "\n",
    "transformList= []\n",
    "transformList.append(transforms.Resize((resize_im, resize_im)))\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence_val=transforms.Compose(transformList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathology = 'Pneumonia'\n",
    "pathology = 'Atelectasis'\n",
    "datasetTrain = Siamese_CheXpertDataSet(image_set_csv,image_set,'train',pathology,transformSequence_train)\n",
    "datasetVal = Siamese_CheXpertDataSet(image_set_csv,image_set,'val',pathology,transformSequence_val)\n",
    "datasetTest = Siamese_CheXpertDataSet(test_set_csv,test_set,'test',pathology,transformSequence_val)\n",
    "\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=batch_size, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=1, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataLoaderTrain))\n",
    "print(len(dataLoaderVal))\n",
    "print(len(dataLoaderTest))\n",
    "print(len(datasetTrain))\n",
    "print(len(datasetVal))\n",
    "print(len(datasetTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()  \n",
    "dataiter = iter(dataLoaderTrain)\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "      def __init__(self, margin):\n",
    "            super(ContrastiveLoss, self).__init__()\n",
    "            self.margin = margin\n",
    "            self.eps = 1e-9\n",
    "\n",
    "      def forward(self, output1, output2, label):\n",
    "#             print(output1.shape)\n",
    "#             print(output2.shape)\n",
    "#             print(label.shape)\n",
    "#             print(label)\n",
    "            # Find the pairwise distance or eucledian distance of two output feature vectors\n",
    "            euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "            # perform contrastive loss calculation with the distance\n",
    "            loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - (euclidean_distance+self.eps), min=0.0), 2))\n",
    "\n",
    "            return loss_contrastive\n",
    "        \n",
    "loss = ContrastiveLoss(margin=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_DenseNet121(nn.Module):\n",
    "    def __init__(self, classNum, pretrained):\n",
    "\n",
    "        super(Siamese_DenseNet121, self).__init__()\n",
    "\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=pretrained, memory_efficient=True)\n",
    "        prevNum = self.densenet121.classifier.in_features\n",
    "        #replacing classifier FC for 1 class\n",
    "        self.densenet121.classifier = nn.Sequential(nn.Linear(prevNum, classNum), nn.ReLU())\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.densenet121(input1)\n",
    "        output2 = self.densenet121(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_ChexpertTrainer():\n",
    " \n",
    "    def train (dataloaderTrain, dataloaderVal, pretrained, classes, batch_size, epoch, resize_im, launchTimestamp, checkpoint):\n",
    "\n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE\n",
    "        #model = DenseNet121(classes, pretrained).cuda()\n",
    "        model = Siamese_DenseNet121(classes, pretrained).cuda()\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         model.to(device)\n",
    "        # model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "        #-------------------- SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "        \n",
    "        #---- Load checkpoint \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "\n",
    "        \n",
    "        #---- TRAIN THE NETWORK\n",
    "        \n",
    "        lossMIN = 100000\n",
    "        \n",
    "        for epochID in range (0, epoch):\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampSTART = timestampDate + '-' + timestampTime\n",
    "                         \n",
    "            Siamese_ChexpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, scheduler, epoch, classes, loss)\n",
    "            lossVal, losstensor = Siamese_ChexpertTrainer.epochVal (model, dataLoaderVal, optimizer, scheduler, epoch, classes, loss)\n",
    "            \n",
    "            timestampTime = time.strftime(\"%H%M%S\")\n",
    "            timestampDate = time.strftime(\"%d%m%Y\")\n",
    "            timestampEND = timestampDate + '-' + timestampTime\n",
    "            \n",
    "            scheduler.step(losstensor)\n",
    "            \n",
    "            if lossVal < lossMIN:\n",
    "                lossMIN = lossVal    \n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'siamese_models/'+'m-' + launchTimestamp + '.pth.tar')\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "            else:\n",
    "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
    "                     \n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for batchID, (input_1, input_2, target) in enumerate (dataLoader):\n",
    "                        \n",
    "            input_1 = Variable(input_1).cuda()\n",
    "            input_2 = Variable(input_2).cuda()\n",
    "            target = Variable(target).cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                         \n",
    "            varOutput_1,varOutput_2 = model.forward(input_1,input_1)\n",
    "            lossvalue = loss(varOutput_1,varOutput_2,target)       \n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    #-------------------------------------------------------------------------------- \n",
    "        \n",
    "    def epochVal (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        lossVal = 0\n",
    "        lossValNorm = 0\n",
    "        \n",
    "        losstensorMean = 0\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            for i, (input_1,input_2, target) in enumerate (dataLoader):\n",
    "\n",
    "                input_1 = Variable(input_1).cuda()\n",
    "                input_2 = Variable(input_2).cuda()\n",
    "                target = Variable(target).cuda()\n",
    "\n",
    "                varOutput_1,varOutput_2 = model.forward(input_1,input_1)\n",
    "\n",
    "                losstensor = loss(varOutput_1,varOutput_2,target)\n",
    "                losstensorMean += float(losstensor)\n",
    "\n",
    "                lossVal += float(losstensor) #or losstensor.item()\n",
    "                lossValNorm += 1\n",
    "\n",
    "            outLoss = lossVal / lossValNorm\n",
    "            losstensorMean = losstensorMean / lossValNorm\n",
    "\n",
    "            return outLoss, losstensorMean\n",
    "    \n",
    "    def computeAUROC (dataGT, dataPRED):\n",
    "        \n",
    "        return roc_auc_score(dataGT, dataPRED)\n",
    "\n",
    "            \n",
    "    def test (dataloaderTest, pathModel, classes, pretrained, batch_size, resize_im, launchTimeStamp, class_names, train_set, transform):   \n",
    "        \n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        df_ones = train_set[train_set[pathology]==1].head(3)\n",
    "        df_minus_ones = train_set[train_set[pathology]==-1].head(3) \n",
    "        df_others = train_set[train_set[pathology]==0].head(3)\n",
    "        \n",
    "        #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n",
    "        model = Siamese_DenseNet121(classes, pretrained).cuda()\n",
    "        \n",
    "        modelCheckpoint = torch.load(pathModel)\n",
    "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "        \n",
    "        outGT = []\n",
    "        outPRED = []\n",
    "        outPRED_ones = 0\n",
    "        outPRED_zeros = 0\n",
    "       \n",
    "        model.eval()        \n",
    "\n",
    "        for i, (input_2, target) in enumerate(dataLoaderTest):\n",
    "            \n",
    "            outGT.extend(target.numpy())\n",
    "            \n",
    "            if i%50 == 0:\n",
    "                print(\"i\")\n",
    "                print(i)\n",
    "            \n",
    "            \n",
    "            for j in range(3):\n",
    "\n",
    "                input_1 = transform(Image.open(df_ones.iloc[j,0]).convert('RGB'))\n",
    "                input_3 = transform(Image.open(df_minus_ones.iloc[j,0]).convert('RGB'))\n",
    "                input_4 = transform(Image.open(df_others.iloc[j,0]).convert('RGB'))\n",
    "\n",
    "                input_1 = Variable(input_1).cuda()\n",
    "                input_2 = Variable(input_2).cuda()\n",
    "                input_3 = Variable(input_3).cuda()\n",
    "                input_4 = Variable(input_4).cuda()\n",
    "                bs, c, h, w = input_2.size()\n",
    "                input_1 = input_1.view(-1, c, h, w)\n",
    "                input_2 = input_2.view(-1, c, h, w)\n",
    "                input_3 = input_3.view(-1, c, h, w)\n",
    "                input_4 = input_3.view(-1, c, h, w)\n",
    "                \n",
    "#                 print(type(target.numpy()))\n",
    "#                 print(type(Variable(target).float()))\n",
    "#                 print(type(outGT))\n",
    "\n",
    "                out_1,out_2 = model(input_1,input_2)\n",
    "                out = F.pairwise_distance(out_1, out_2)\n",
    "#                 print(type(out))\n",
    "#                 print(type(out.item()))\n",
    "#                 print(out.shape)\n",
    "                outPRED_ones += out.item()\n",
    "\n",
    "                out_1,out_2 = model(input_3,input_2)\n",
    "                out = F.pairwise_distance(out_1, out_2)\n",
    "                outPRED_ones += out.item()\n",
    "\n",
    "                out_1,out_2 = model(input_4,input_2)\n",
    "                out = F.pairwise_distance(out_1, out_2)\n",
    "#                 print(out.shape)\n",
    "                outPRED_zeros += out.item()\n",
    "            \n",
    "            outPRED_ones = np.exp(outPRED_ones) / (np.exp(outPRED_ones)+np.exp(outPRED_zeros))\n",
    "            outPRED_zeros = np.exp(outPRED_zeros) / (np.exp(outPRED_ones)+np.exp(outPRED_zeros))\n",
    "            \n",
    "#             print(\"outpred_ones\")\n",
    "#             print(outPRED_ones)\n",
    "            \n",
    "            if outPRED_ones > outPRED_zeros:\n",
    "                outPRED.append(1)\n",
    "                #outPRED = torch.cat((outPRED, 1), 0)\n",
    "            else:\n",
    "                outPRED.append(0)\n",
    "                #outPRED = torch.cat((outPRED, 0), 0)\n",
    "        \n",
    "        print(len(outPRED))\n",
    "        print(len(outGT))\n",
    "        print(outPRED[0])\n",
    "        print(outGT[0])\n",
    "\n",
    "        aurocIndividual = Siamese_ChexpertTrainer.computeAUROC(outGT, outPRED)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        \n",
    "        print ('AUROC mean ', aurocMean)\n",
    "        \n",
    "#         for i in range (0, len(aurocIndividual)):\n",
    "#             print (class_names[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "        return outGT, outPRED, aurocMean\n",
    "#-------------------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampTime = time.strftime(\"%H%M%S\")\n",
    "timestampDate = time.strftime(\"%d%m%Y\")\n",
    "timestampLaunch = timestampDate + '-' + timestampTime\n",
    "       \n",
    "pathModel = 'm-' + timestampLaunch + '.pth.tar'\n",
    "    \n",
    "print ('Training NN architecture')\n",
    "Siamese_ChexpertTrainer.train(dataLoaderTrain, dataLoaderVal, pretrained, classes, batch_size, epochs, resize_im, timestampLaunch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())   # --> 0\n",
    "print(torch.cuda.is_available())   # --> False\n",
    "print(torch.version.cuda)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
